{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import csv\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "with open(f\"C:/Users/Sayali Hatwar/OneDrive/Desktop/Project - 2(Linkedln scrapping)/LinkedIn {date.today()}.csv\",'a+',encoding='UTF8' ,newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\"Date\",\"Job_Name\",\"Company\",'Loaction','Job_type','Employees','Followers','Applicant','Industry','Invovlement']\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "# Creating login page and getting jobs page\n",
    "\n",
    "email = \"***************\"\n",
    "password= \"*************\"\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "username = driver.find_element(\"id\",\"username\")\n",
    "\n",
    "username.send_keys(email)\n",
    "\n",
    "passw = driver.find_element(\"id\",\"password\")\n",
    "passw.send_keys(password)\n",
    "\n",
    "driver.find_element(\"xpath\",\"//button[@type='submit']\").click()\n",
    "\n",
    "# heading to jobs page\n",
    "# type job url according to preference\n",
    "time.sleep(5)\n",
    "job_url=\"https://www.linkedin.com/jobs/DataScientist\"\n",
    "driver.get(job_url)\n",
    "time.sleep(5)\n",
    "lnk = set()\n",
    "print(\"going to next page\")\n",
    "\n",
    "\n",
    "# Changing pages and getting all links of pages\n",
    "\n",
    "for i in range(2,41):\n",
    "    driver.find_element(\"xpath\",f\"//button[@aria-label='Page {i}']\").click()\n",
    "\n",
    "    print(i)\n",
    "    src = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(src,'html.parser')\n",
    "        \n",
    "    anc= soup.find_all('a')\n",
    "            \n",
    "    for i in anc:\n",
    "        lnk.add(i.get('href'))\n",
    "            \n",
    "    print('link_added')\n",
    "    time.sleep(3)\n",
    "print(lnk)\n",
    "\n",
    "\n",
    "# link added to lnk variable now selecting only jobs link and adding to XT variable\n",
    "\n",
    "\n",
    "xt = []\n",
    "for i in lnk:\n",
    "    if i[:6] == '/jobs/':     \n",
    "        m = 'https://www.linkedin.com/'+i   \n",
    "        xt.append(m) \n",
    "        \n",
    "        \n",
    "# Links added now getting data from all the pages       \n",
    "    \n",
    "for i in xt:\n",
    "    try:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        src2 = driver.page_source\n",
    "        soup2 = BeautifulSoup(src2,'html.parser')\n",
    "\n",
    "        try:\n",
    "            title = soup2.find('h1',class_='t-24 t-bold jobs-unified-top-card__job-title').text.replace('\\n','').strip()\n",
    "        except:\n",
    "            try:\n",
    "                title = soup2.find('a',class_='ember-view t-black t-normal').text.replace('\\n','').strip()\n",
    "            except :\n",
    "                title = 'Null'\n",
    "\n",
    "        try:\n",
    "            company = soup2.find('span',class_='jobs-unified-top-card__company-name').text.replace('\\n','').strip()\n",
    "        except:\n",
    "            try:\n",
    "                company = soup2.find('a',class_='ember-view t-black t-normal').text.replace('\\n','').strip()\n",
    "            except :\n",
    "                company = 'Null'\n",
    "\n",
    "\n",
    "        location = soup2.find('span',class_='jobs-unified-top-card__bullet').text.replace('\\n','').strip()\n",
    "        typ = soup2.find('span',class_='jobs-unified-top-card__workplace-type').text.replace('\\n','').strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            employees = soup2.find('span',class_='jobs-company__inline-information').text.replace('\\n','').strip() \n",
    "        except:\n",
    "            employees = 'Null'\n",
    "\n",
    "\n",
    "        try:\n",
    "            followers = soup2.find('div',class_='artdeco-entity-lockup__subtitle ember-view t-16').text.replace('\\n','').strip().replace('\\n','').strip()\n",
    "        except:\n",
    "            try:\n",
    "                followers = soup2.find('div',class_='artdeco-entity-lockup__content ember-view flex-grow-1').text.replace('\\n','').strip()\n",
    "            except:\n",
    "                followers = 'Null'  \n",
    "\n",
    "        try:\n",
    "            applicant = soup2.find('span',class_='jobs-unified-top-card__applicant-count').text.replace('\\n','').strip() \n",
    "        except:\n",
    "            try:\n",
    "                applicant = soup2.find('span',class_='jobs-unified-top-card__subtitle-secondary-grouping t-black--light').text.replace('\\n','').strip()\n",
    "            except:\n",
    "                applicant = 'Null'\n",
    "\n",
    "\n",
    "\n",
    "        industry = soup2.find('div',class_='t-14 mt5')\n",
    "        for i in industry:\n",
    "            industry = i.text.replace('\\n','').strip()\n",
    "            break\n",
    "\n",
    "        involvement = soup2.find('li',class_='jobs-unified-top-card__job-insight').text.replace('\\n','').strip()\n",
    "        Date = date.today()\n",
    "\n",
    "        rows = [Date,title,company,location,typ,employees,followers,applicant,industry,involvement]\n",
    "        print(rows)\n",
    "\n",
    "# write the file name as per choice\n",
    "        with open(f\"C:/Users/Sayali Hatwar/OneDrive/Desktop/Project - 2(Linkedln scrapping)/LinkedIn {date.today()}.csv\",'a+',encoding='UTF8' ,newline='') as f:\n",
    "            writer = csv.writer(f)     \n",
    "            writer.writerow(rows)\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
